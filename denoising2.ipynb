{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "mount_file_id": "199njqqnnJUtv_VyBkWhxtDY8wLaLcfCC",
      "authorship_tag": "ABX9TyNrOWKy1AboMNHylsNq6oVM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eunchae-L/Audio_Denoising/blob/main/denoising2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5aOIqAwauXk",
        "outputId": "199279c3-25c7-43db-a34c-96ee42d1bfd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MP-SENet'...\n",
            "remote: Enumerating objects: 797, done.\u001b[K\n",
            "remote: Counting objects: 100% (141/141), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 797 (delta 128), reused 104 (delta 104), pack-reused 656\u001b[K\n",
            "Receiving objects: 100% (797/797), 477.72 MiB | 15.87 MiB/s, done.\n",
            "Resolving deltas: 100% (203/203), done.\n",
            "Updating files: 100% (247/247), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/yxlu-0102/MP-SENet.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive에 있는 zip 파일의 경로를 설정\n",
        "zip_file_path = '/content/drive/MyDrive/고려대학교/Audio_detection/denoising/Data_Sources/clean_trainset_wav.zip'  # zip 파일의 경로로 변경\n",
        "\n",
        "# 압축 해제할 경로 설정\n",
        "extract_path = '/content/MP-SENet/VoiceBank+DEMAND/wavs_clean'  # 압축을 해제할 경로로 변경\n",
        "\n",
        "# 디렉토리 생성 (존재하지 않는 경우)\n",
        "import os\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# 압축 해제\n",
        "!unzip {zip_file_path} -d {extract_path}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8jyDePANdV2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Drive에 있는 zip 파일의 경로를 설정\n",
        "zip_file_path = '/content/drive/MyDrive/고려대학교/Audio_detection/denoising/Data_Sources/noisy_trainset_wav.zip'  # zip 파일의 경로로 변경\n",
        "\n",
        "# 압축 해제할 경로 설정\n",
        "extract_path = '/content/MP-SENet/VoiceBank+DEMAND/wavs_noisy'  # 압축을 해제할 경로로 변경\n",
        "\n",
        "# 디렉토리 생성 (존재하지 않는 경우)\n",
        "import os\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "# 압축 해제\n",
        "!unzip {zip_file_path} -d {extract_path}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3Ly0Mj5H7_R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa\n",
        "!pip install soundfile\n",
        "!pip install torch\n",
        "!pip install torchaudio\n",
        "!pip install torchvision\n",
        "!pip install einops\n",
        "!pip install pesq\n",
        "!pip install tensorboard\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Ctm3KITrl5wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "\n",
        "def resample_wav(input_dir, output_dir, target_sr=16000):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    for root, dirs, files in os.walk(input_dir):\n",
        "        for file in files:\n",
        "            if file.endswith('.wav'):\n",
        "                input_path = os.path.join(root, file)\n",
        "                output_path = os.path.join(output_dir, file)\n",
        "\n",
        "                # Load the audio file\n",
        "                audio, sr = librosa.load(input_path, sr=None)\n",
        "\n",
        "                # Resample the audio to target sample rate\n",
        "                resampled_audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
        "\n",
        "                # Save the resampled audio to the output directory\n",
        "                sf.write(output_path, resampled_audio, target_sr)\n",
        "                print(f'Resampled {input_path} to {output_path}')\n",
        "\n",
        "input_dir_clean = '/content/MP-SENet/VoiceBank+DEMAND/wavs_clean'\n",
        "output_dir_clean = '/content/MP-SENet/VoiceBank+DEMAND/wavs_clean_16k'\n",
        "input_dir_noisy = '/content/MP-SENet/VoiceBank+DEMAND/wavs_noisy'\n",
        "output_dir_noisy = '/content/MP-SENet/VoiceBank+DEMAND/wavs_noisy_16k'\n",
        "\n",
        "# Resample clean speech\n",
        "resample_wav(input_dir_clean, output_dir_clean)\n",
        "\n",
        "# Resample noisy speech\n",
        "resample_wav(input_dir_noisy, output_dir_noisy)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7d0giyfvl8MO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from models.generator import MPNet\n",
        "from utils import load_checkpoint\n",
        "from datasets.dataset import mag_pha_stft, mag_pha_istft\n",
        "from env import AttrDict\n",
        "\n",
        "def main():\n",
        "    print('Initializing Inference Process..')\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--input_noisy_wavs_dir', default='/content/drive/MyDrive/고려대학교/Audio_detection/data/test')\n",
        "    parser.add_argument('--output_dir', default='/content/drive/MyDrive/고려대학교/Audio_detection/data/cleaned')\n",
        "    parser.add_argument('--checkpoint_file', required=True)\n",
        "    a = parser.parse_args()\n",
        "\n",
        "    config_file = os.path.join(os.path.split(a.checkpoint_file)[0], 'config.json')\n",
        "    with open(config_file) as f:\n",
        "        data = f.read()\n",
        "\n",
        "    global h\n",
        "    json_config = json.loads(data)\n",
        "    h = AttrDict(json_config)\n",
        "\n",
        "    torch.manual_seed(h.seed)\n",
        "    global device\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(h.seed)\n",
        "        device = torch.device('cuda')\n",
        "    else:\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    inference(a)\n",
        "\n",
        "def inference(a):\n",
        "    model = MPNet(h).to(device)\n",
        "\n",
        "    state_dict = load_checkpoint(a.checkpoint_file, device)\n",
        "    model.load_state_dict(state_dict['generator'])\n",
        "\n",
        "    ogg_files = glob.glob(os.path.join(a.input_noisy_wavs_dir, '*.ogg'))\n",
        "\n",
        "    os.makedirs(a.output_dir, exist_ok=True)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for ogg_file in ogg_files:\n",
        "            index = os.path.splitext(os.path.basename(ogg_file))[0]\n",
        "            print(index)\n",
        "\n",
        "            # OGG 파일을 WAV로 변환 및 로드\n",
        "            y, sr = librosa.load(ogg_file, sr=h.sampling_rate)\n",
        "            noisy_wav = torch.FloatTensor(y).to(device)\n",
        "            norm_factor = torch.sqrt(len(noisy_wav) / torch.sum(noisy_wav ** 2.0)).to(device)\n",
        "            noisy_wav = (noisy_wav * norm_factor).unsqueeze(0)\n",
        "            noisy_amp, noisy_pha, noisy_com = mag_pha_stft(noisy_wav, h.n_fft, h.hop_size, h.win_size, h.compress_factor)\n",
        "            amp_g, pha_g, com_g = model(noisy_amp, noisy_pha)\n",
        "            audio_g = mag_pha_istft(amp_g, pha_g, h.n_fft, h.hop_size, h.win_size, h.compress_factor)\n",
        "            audio_g = audio_g / norm_factor\n",
        "\n",
        "            output_file = os.path.join(a.output_dir, index + '.wav')\n",
        "            sf.write(output_file, audio_g.squeeze().cpu().numpy(), h.sampling_rate, 'PCM_16')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "0eStcq7t0Hsy",
        "outputId": "d76826b2-176d-4d08-efe8-a53210691633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Inference Process..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: colab_kernel_launcher.py [-h] [--input_noisy_wavs_dir INPUT_NOISY_WAVS_DIR]\n",
            "                                [--output_dir OUTPUT_DIR] --checkpoint_file CHECKPOINT_FILE\n",
            "colab_kernel_launcher.py: error: the following arguments are required: --checkpoint_file\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "2",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python inference.py --checkpoint_file /content/MP-SENet/best_ckpt/g_best"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFwoyFz90Xhj",
        "outputId": "6312398e-88cc-45dd-ff8f-5db3fae890c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Inference Process..\n",
            "Loading '/content/MP-SENet/best_ckpt/g_best'\n",
            "Complete.\n",
            "p232_001\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/MP-SENet/inference.py\", line 93, in <module>\n",
            "    main()\n",
            "  File \"/content/MP-SENet/inference.py\", line 89, in main\n",
            "    inference(a)\n",
            "  File \"/content/MP-SENet/inference.py\", line 48, in inference\n",
            "    noisy_wav, _ = librosa.load(os.path.join(a.input_noisy_wavs_dir, index+'.wav'), h.sampling_rate)\n",
            "TypeError: load() takes 1 positional argument but 2 were given\n"
          ]
        }
      ]
    }
  ]
}